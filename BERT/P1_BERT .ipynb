{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aq5vF9wYZal5"
   },
   "outputs": [],
   "source": [
    "#Please Execute this notebook in Google Colab. I am not sure if this would work perfectly fine on local jupytr notebook as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "9c_rKmGk7Jto",
    "outputId": "aeda188c-5558-4b7d-a877-4dc44a5172c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "drive  sample_data\n",
      "/content\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!ls\n",
    "!pwd\n",
    "!cd /content/drive\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "FSXDhDkoOUbr",
    "outputId": "49bbea0b-2d84-41ae-8f57-b36fe40b2132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/NLP/bert-for-tf2/requirements.txt (line 4)) (0.9.7)\n",
      "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/NLP/bert-for-tf2/requirements.txt (line 5)) (0.8.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->-r /content/drive/My Drive/NLP/bert-for-tf2/requirements.txt (line 5)) (1.18.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->-r /content/drive/My Drive/NLP/bert-for-tf2/requirements.txt (line 5)) (4.41.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r '/content/drive/My Drive/NLP/bert-for-tf2/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAmnyu5ADgaI"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bsJfS1KyTWNk"
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/content/drive/My Drive/NLP/bert-for-tf2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AbkLQ_QqTROJ"
   },
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert.model import BertModelLayer\n",
    "from bert.loader import params_from_pretrained_ckpt, load_stock_weights\n",
    "from bert.tokenization.bert_tokenization import FullTokenizer\n",
    "\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "basePath = '/content/drive/My Drive/NLP/input/bert-pretrained-models/multi_cased_L-12_H-768_A-12/multi_cased_L-12_H-768_A-12/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vPvT9QHEJFO"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/content/drive/My Drive/NLP/P1_train.csv\")\n",
    "test = pd.read_csv(\"/content/drive/My Drive/NLP/P1_test.csv\")\n",
    "\n",
    "data = train['sentence'].values\n",
    "labels = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6oSe4rAElUS"
   },
   "outputs": [],
   "source": [
    "sentenceLength = 256\n",
    "totalClasses = 3\n",
    "tokenPath = basePath + 'vocab.txt'\n",
    "tokenizer = FullTokenizer(tokenPath, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSGn-kt9E4Ju"
   },
   "outputs": [],
   "source": [
    "def getTokens(data):\n",
    "  temp = []\n",
    "  for row in data['sentence']:\n",
    "      temp.append( [\"[CLS]\"] + tokenizer.tokenize(str(row)) + [\"[SEP]\"] )\n",
    "  tokens = list(map(tokenizer.convert_tokens_to_ids, temp))\n",
    "  tokens = map(lambda tids: tids + [0] * (sentenceLength - len(tids)), tokens)\n",
    "  tokens = [tf.convert_to_tensor(xi) for xi in list(tokens)]\n",
    "  return tokens\n",
    "\n",
    "x_train = tf.convert_to_tensor(getTokens(train))\n",
    "x_test = tf.convert_to_tensor(getTokens(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mm5RlaACE6Ry"
   },
   "outputs": [],
   "source": [
    "bert_params = params_from_pretrained_ckpt(basePath)\n",
    "bert_layer = BertModelLayer.from_params(bert_params, name=\"bert\")\n",
    "bert_layer.apply_adapter_freeze()\n",
    "\n",
    "def create_model(sentenceLength, classes):\n",
    "    inputShape = Input(shape=(sentenceLength,), dtype='int32', name='input')\n",
    "    bertLayer = bert_layer(inputShape)\n",
    "    cls_out = Lambda(lambda seq: seq[:, 0, :])(bertLayer)\n",
    "    dropout = Dropout(0.1)(cls_out)\n",
    "    fc_1 = Dense(64, activation=tf.nn.relu)(dropout)\n",
    "    dr_2 = Dropout(0.2)(fc_1)\n",
    "    finalOutputShape = Dense(classes, activation='softmax')(dr_2)\n",
    "    model = Model(inputShape, finalOutputShape)    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "id": "gz0zchy7FCNq",
    "outputId": "acdfca3b-da1e-4bbb-b461-d3843cfeba16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading 196 BERT weights from: /content/drive/My Drive/NLP/input/bert-pretrained-models/multi_cased_L-12_H-768_A-12/multi_cased_L-12_H-768_A-12/bert_model.ckpt into <bert.model.BertModelLayer object at 0x7fe63e421da0> (prefix:bert). Count of weights not found in the checkpoint was: [0]. Count of weights with mismatched shape: [0]\n",
      "Unused weights from checkpoint: \n",
      "\tbert/embeddings/token_type_embeddings\n",
      "\tbert/pooler/dense/bias\n",
      "\tbert/pooler/dense/kernel\n",
      "\tcls/predictions/output_bias\n",
      "\tcls/predictions/transform/LayerNorm/beta\n",
      "\tcls/predictions/transform/LayerNorm/gamma\n",
      "\tcls/predictions/transform/dense/bias\n",
      "\tcls/predictions/transform/dense/kernel\n",
      "\tcls/seq_relationship/output_bias\n",
      "\tcls/seq_relationship/output_weights\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, 256)]             0         \n",
      "_________________________________________________________________\n",
      "bert (BertModelLayer)        (None, 256, 768)          177261312 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                49216     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 177,310,723\n",
      "Trainable params: 49,411\n",
      "Non-trainable params: 177,261,312\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = create_model(sentenceLength, totalClasses)\n",
    "model.build(input_shape=(None, sentenceLength))\n",
    "load_stock_weights(bert_layer, basePath+\"bert_model.ckpt\")\n",
    "def flatten_layers(bert_layer):\n",
    "    if isinstance(bert_layer, keras.layers.Layer):\n",
    "        yield bert_layer\n",
    "    for layer in bert_layer._layers:\n",
    "        for sub_layer in flatten_layers(layer):\n",
    "            yield sub_layer\n",
    "\n",
    "def getLayerInfo(name):\n",
    "  if layer.name in [\"LayerNorm\", \"adapter-down\", \"adapter-up\"]:\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "for layer in flatten_layers(bert_layer):\n",
    "  if getLayerInfo(layer.name): layer.trainable = True\n",
    "  else: layer.trainable = False\n",
    "\n",
    "bert_layer.embeddings_layer.trainable = False\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.00001), metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjeyj_4MLcrx"
   },
   "outputs": [],
   "source": [
    "# checkpointName = \"bert_fine-tuning.ckpt\"\n",
    "# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpointName, save_weights_only=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "bKjXnJBVRPQa",
    "outputId": "494e77d9-9d90-4bd2-c65c-16868d21bdb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.1473 - accuracy: 0.4066\n",
      "Epoch 2/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.1125 - accuracy: 0.4313\n",
      "Epoch 3/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0961 - accuracy: 0.4277\n",
      "Epoch 4/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0786 - accuracy: 0.4410\n",
      "Epoch 5/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0896 - accuracy: 0.4157\n",
      "Epoch 6/50\n",
      "52/52 [==============================] - 19s 375ms/step - loss: 1.1155 - accuracy: 0.4000\n",
      "Epoch 7/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0760 - accuracy: 0.4265\n",
      "Epoch 8/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0851 - accuracy: 0.4229\n",
      "Epoch 9/50\n",
      "52/52 [==============================] - 20s 377ms/step - loss: 1.0954 - accuracy: 0.4223\n",
      "Epoch 10/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0964 - accuracy: 0.4066\n",
      "Epoch 11/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0791 - accuracy: 0.4193\n",
      "Epoch 12/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0743 - accuracy: 0.4331\n",
      "Epoch 13/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0572 - accuracy: 0.4410\n",
      "Epoch 14/50\n",
      "52/52 [==============================] - 19s 373ms/step - loss: 1.0810 - accuracy: 0.4133\n",
      "Epoch 15/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0815 - accuracy: 0.4139\n",
      "Epoch 16/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0778 - accuracy: 0.4072\n",
      "Epoch 17/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0776 - accuracy: 0.4151\n",
      "Epoch 18/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0777 - accuracy: 0.4199\n",
      "Epoch 19/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0739 - accuracy: 0.4283\n",
      "Epoch 20/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0779 - accuracy: 0.4205\n",
      "Epoch 21/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0811 - accuracy: 0.4084\n",
      "Epoch 22/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0626 - accuracy: 0.4114\n",
      "Epoch 23/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0629 - accuracy: 0.4199\n",
      "Epoch 24/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0621 - accuracy: 0.4229\n",
      "Epoch 25/50\n",
      "52/52 [==============================] - 19s 375ms/step - loss: 1.0699 - accuracy: 0.4283\n",
      "Epoch 26/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0578 - accuracy: 0.4265\n",
      "Epoch 27/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0498 - accuracy: 0.4500\n",
      "Epoch 28/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0464 - accuracy: 0.4584\n",
      "Epoch 29/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0656 - accuracy: 0.4271\n",
      "Epoch 30/50\n",
      "52/52 [==============================] - 19s 374ms/step - loss: 1.0531 - accuracy: 0.4410\n",
      "Epoch 31/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0477 - accuracy: 0.4217\n",
      "Epoch 32/50\n",
      "52/52 [==============================] - 19s 373ms/step - loss: 1.0508 - accuracy: 0.4440\n",
      "Epoch 33/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0602 - accuracy: 0.4265\n",
      "Epoch 34/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0505 - accuracy: 0.4325\n",
      "Epoch 35/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0638 - accuracy: 0.4247\n",
      "Epoch 36/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0590 - accuracy: 0.4343\n",
      "Epoch 37/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0437 - accuracy: 0.4247\n",
      "Epoch 38/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0522 - accuracy: 0.4223\n",
      "Epoch 39/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0661 - accuracy: 0.4157\n",
      "Epoch 40/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0490 - accuracy: 0.4524\n",
      "Epoch 41/50\n",
      "52/52 [==============================] - 19s 373ms/step - loss: 1.0540 - accuracy: 0.4211\n",
      "Epoch 42/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0539 - accuracy: 0.4277\n",
      "Epoch 43/50\n",
      "52/52 [==============================] - 19s 371ms/step - loss: 1.0495 - accuracy: 0.4343\n",
      "Epoch 44/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0391 - accuracy: 0.4560\n",
      "Epoch 45/50\n",
      "52/52 [==============================] - 19s 373ms/step - loss: 1.0526 - accuracy: 0.4265\n",
      "Epoch 46/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0388 - accuracy: 0.4386\n",
      "Epoch 47/50\n",
      "52/52 [==============================] - 19s 373ms/step - loss: 1.0364 - accuracy: 0.4645\n",
      "Epoch 48/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0362 - accuracy: 0.4271\n",
      "Epoch 49/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0361 - accuracy: 0.4440\n",
      "Epoch 50/50\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 1.0458 - accuracy: 0.4367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe59c2e9b38>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, tf.convert_to_tensor(train['label']), epochs=50, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8aEW60wjV0m3"
   },
   "outputs": [],
   "source": [
    "results = model.predict(x_test)\n",
    "results = results.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "weSNzg4Tk6ok"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6Y0qhFtGmF8q",
    "outputId": "701f2738-608b-4057-e757-44d867b7e8c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.465592972181552\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(test['label'], results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oV68ljmamHlv",
    "outputId": "4d0e2255-1ab5-425d-c150-72e0afd6cec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.289177875760451\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(test['label'], results, average = 'macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLSusaNDyteH"
   },
   "outputs": [],
   "source": [
    "#Reference: https://www.kaggle.com/seriousran/twitter-sentiment-analysis-using-bert-in-tf-2/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ei4Z2vYmaPaY"
   },
   "outputs": [],
   "source": [
    "test['predicted_label'] = pd.Series(results)\n",
    "test.to_csv('testing_output_proposed.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9gm7Fmt_ksiP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
